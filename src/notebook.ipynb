{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python packages\n",
    "import os\n",
    "\n",
    "\n",
    "# Import installed libraries\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "print('Tensorflow version', tf.version.VERSION)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "EPOCHS = 20\n",
    "IMG_HEIGHT = 150\n",
    "IMG_WIDHT = 150\n",
    "BATCH_SIZE = 32\n",
    "INPUT_SHAPE = (150, 150, 3)\n",
    "\n",
    "# File paths\n",
    "DESKTOP = '/Users/abhidgd/Desktop/mcmaster/cps-dl/fire-detection'\n",
    "DATASET = DESKTOP + '/data/'\n",
    "FIRE_IMG_PATH = DATASET + '/1'\n",
    "NO_FIRE_IMG_PATH = DATASET + '/0'\n",
    "FIRE_IMG = FIRE_IMG_PATH + '/1.jpg'\n",
    "RAND_IMG = NO_FIRE_IMG_PATH + '/1.jpg'\n",
    "\n",
    "# Output paths\n",
    "CHECKPOINT_FILEPATH = './fire-detection/temp/checkpoint'\n",
    "SAVE_KERAS_MODEL = '/model/retrained_imagenet.keras'\n",
    "SAVE_H5_MODEL = '/model/retrained_imagenet.h5'\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\"[INFO] : Constants defined\")\n",
    "print(\"[DEBUG] : Batch size = \", BATCH_SIZE)\n",
    "print(\"[DEBUG] : Epochs = \", EPOCHS)\n",
    "print(\"[DEBUG] : Input image height = \", IMG_HEIGHT)\n",
    "print(\"[DEBUG] : Input image width = \", IMG_WIDHT)\n",
    "print(\"[DEBUG] : Input image shape = \", INPUT_SHAPE)\n",
    "print(f\"[DEBUG] : Saving model checkpoints to {CHECKPOINT_FILEPATH}\",)\n",
    "print(f\"[DEBUG] : Saving keras extension model to {SAVE_KERAS_MODEL}\",)\n",
    "print(f\"[DEBUG] : If required saving HDF5 extension model to {SAVE_H5_MODEL}\",)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_files_in_folders(parent_folder) -> dict:\n",
    "    \"\"\"Count files in given folder. Return dictionary with folder and count.\"\"\"\n",
    "    folder_names = os.listdir(parent_folder)\n",
    "    file_count = {}\n",
    "\n",
    "    for folder_name in folder_names:\n",
    "        folder_path = os.path.join(parent_folder, folder_name)\n",
    "        if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
    "            file_count[folder_name] = len(\n",
    "                [file for file in os.listdir(folder_path) if os.path.isfile(\n",
    "                    os.path.join(folder_path, file)\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            file_count[folder_name] = 0\n",
    "\n",
    "    return file_count\n",
    "\n",
    "\n",
    "def load_and_prep_image(filename, img_shape = 300):\n",
    "    \"\"\"Funtion to read image and transform image to tensor.\"\"\"\n",
    "    img = tf.io.read_file(filename) #read image\n",
    "    img = tf.image.decode_image(img) # decode the image to a tensor\n",
    "    img = tf.image.resize(img, size = [img_shape, img_shape]) # resize the image\n",
    "    return img\n",
    "\n",
    "\n",
    "def pred_and_plot(model, filename, class_names):\n",
    "    \"\"\"Funtion to read image and give desired output with image.\n",
    "\n",
    "    Imports an image located at filename, makes a prediction on it with\n",
    "    a trained model and plots the image with the predicted class as the title.\n",
    "    \"\"\"\n",
    "    # Import the target image and preprocess it\n",
    "    img = load_and_prep_image(filename)\n",
    "    \n",
    "    # Make a prediction\n",
    "    pred = model.predict(tf.expand_dims(img, axis=0))\n",
    "    \n",
    "    if len(pred[0]) > 1: # check for multi-class\n",
    "        # if more than one output, take the max\n",
    "        pred_class = class_names[pred.argmax()]\n",
    "    else:\n",
    "        # if only one output, round\n",
    "        pred_class = class_names[int(tf.round(pred)[0][0])]\n",
    "\n",
    "    # Plot the image and predicted class\n",
    "    sh_image = plt.imread(filename)\n",
    "    plt.imshow(sh_image)\n",
    "    plt.title(f\"Prediction: {pred_class}\")\n",
    "    plt.axis(False)\n",
    "\n",
    "    # specifying path to sample image from list of test images.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        DATASET,\n",
    "        validation_split = 0.2,\n",
    "        image_size=(IMG_HEIGHT, IMG_WIDHT),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        subset=\"training\",\n",
    "        seed=50,\n",
    ")\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        DATASET,\n",
    "        validation_split=0.2,\n",
    "        image_size=(IMG_HEIGHT, IMG_WIDHT),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        subset=\"validation\",\n",
    "        seed=50,\n",
    ")\n",
    "\n",
    "# Verify data\n",
    "print(\"-\" * 80)\n",
    "print(\"[INFO] : BatchDataset type objects returned.\")\n",
    "print(\"[INFO] : Class names:\", train_ds.class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 1\n",
    "for (image, label) in train_ds.take(1):\n",
    "    # Plot the first 9 images\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(image[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(train_ds.class_names[label[i]])\n",
    "        plt.axis(\"Off\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print('[INFO] : Showing image from training dataset with 20% validation data.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use caching and prefetching to optimize loading speed\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print('[INFO] : Autotune setup complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce randomly flipped images to introduce generalization and reduce\n",
    "# overfitting\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.1),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pre-trained Resnet-50 layers model to train on our fire-dataset\n",
    "# here we are setting include_top as False, as we will add our own dense layers after resnet 50 last layer\n",
    "pre_trained_resnet_50 = tf.keras.applications.ResNet50(include_top = False,\n",
    "                                                      input_shape = INPUT_SHAPE,\n",
    "                                                      pooling = 'avg',\n",
    "                                                      classes = 100,\n",
    "                                                      weights = 'imagenet')\n",
    "\n",
    "# Here we want last 10 layers to be trainable so freezing first 40 layers\n",
    "x = 0\n",
    "for layer in pre_trained_resnet_50.layers:\n",
    "    layer.trainable = False\n",
    "    x+=1\n",
    "    if x == 39:\n",
    "        break\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\"[INFO] : Completed importing pre trained RESNET50 model weights.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding extra Dense layers after Resnet 50 last layer, we do this to increase\n",
    "# our models capability to categorise image as having fire or not having fire\n",
    "model = Sequential()\n",
    "model.add(pre_trained_resnet_50)\n",
    "model.add(Dense(2048, activation='relu'))\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(8000, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4000, activation='relu'))\n",
    "model.add(Dense(2000, activation='relu'))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using tensorflow's learning-rate-scheduler to change learning rate at each epoch\n",
    "# this will help us to find the best learning rate for our model\n",
    "learning_rate_callback = tf.keras.callbacks.LearningRateScheduler(\n",
    "            lambda epoch: 1e-8 * 10**(epoch/20)\n",
    ")\n",
    "print(\"-\" * 80)\n",
    "print(\"[INFO] : Learning rate callback scheduled.\")\n",
    "\n",
    "# Using tensorflow's ModelCheckpoint to save best model having less validation loss\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = CHECKPOINT_FILEPATH, monitor = 'val_loss',\n",
    "    save_best_only = True,\n",
    ")\n",
    "print(\"-\" * 80)\n",
    "print(\"[INFO] : Model checkpoint callback scheduled.\")\n",
    "\n",
    "# Using Adam optimizer to optimize our model to better learn on our dataset\n",
    "model.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(),\n",
    "    loss = 'binary_crossentropy',\n",
    "    metrics = 'accuracy'\n",
    ")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\"[INFO] : Model compiled using Adam optimizer.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "print(\"-\" * 80)\n",
    "print(\"[INFO] : Starting model fitting with callbacks.\")\n",
    "# Now time to train our model on fire dataset\n",
    "model_hist = model.fit(train_ds, validation_data = test_ds,\n",
    "        epochs = EPOCHS,\n",
    "        callbacks = [learning_rate_callback,  model_checkpoint_callback,],\n",
    ")\n",
    "\n",
    "# Save model parameters\n",
    "try:\n",
    "    print(\"[INFO] : Trying to save a .keras format model.\")\n",
    "    model.save(SAVE_KERAS_MODEL)\n",
    "    print(\"[INFO] : Saved .keras model successfully.\")\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    print(\"[INFO] : Trying to save a HDF5 model format.\")\n",
    "    model.save(SAVE_H5_MODEL,)\n",
    "    print(\"[INFO] : Saved .h5 model successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook notebook.ipynb to script\n",
      "[NbConvertApp] Writing 8385 bytes to notebook.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script notebook.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "collabfilterenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
